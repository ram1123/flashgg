Quick readme to use the jackknife machinery
-------------------------------------------

In order to use the machinery you would need the following inputs:
- signal and background workspaces for the two analyses
- data event dumps for the two analyses

Please feel free to improve the machinery, but please before committing check with Pasquale and other people
using it.

Then you should take the following steps:

1. Convert the event dumps to mini-trees and generate a json file defining the partitions.
This is done using the dump2minitree.py script. See "./dump2minitree.py --help" to see the different options.
./dump2minitree.py -d 100 -t <list1>.txt <list2>.txt

This will generate <list1>.root and <list2>.root and a file partitions.json.

2. Split the minitree to generate the jackknife samples (and the corresponding signal workspaces)
This is done with splitMiniTree.py. Check splitMiniTree.py --help for more details.

3. Set up folders to be used to run combine. This is semiauthomatic.
The setupForCombine.sh scripts will make folders containing the background workspace for each jackknife sample.
You will need to copy (or better link) the datacard and the signal workspace to each folder.

4. Run combine to get the quantities that you want. 

5. Analyze the results. 
If you use the scritps ResultsScripts/doPvalue.sh and ResultsScripts/doBestMu.sh, you 
can then use the script ./getResults.sh to get the summary of the analysis in cvs format and ./plotResults.py to 
analyze the csv file.



 


